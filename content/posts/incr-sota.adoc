= xref:.[Incremental Computation SOTAs]
:showtitle:
:lang: en
:stem: latexmath

== Incremental Parsing

=== Incremental Packrat Parsing

Zachary Yedidia and Stephen Chong. 2021. Fast incremental PEG parsing. In Proceedings of the 14th ACM SIGPLAN International Conference on Software Language Engineering (SLE 2021). Association for Computing Machinery, New York, NY, USA, 99–112. https://doi.org/10.1145/3486608.3486900

==== Implementation Difficulty
1. Interval Tree: balanced binary search tree (AVL in prototype). Looks fine.
2. Tree Memo: a stack, scan and insert after each iteration. Looks fine.
3. Parser Machine: an abstract machine for parsing. Can be easy if supported features are limited.

==== Note

.Naive Incremental Packrat Parsing Algorithm
[source]
----
M ← a new memoization table
s ← the initial input string
G ← the grammar
When an edit operation e occurs
  s, M ← ApplyEdit(s, M, e)
  M, res ← Parse(G, s, M)
----

Memo entries contain:

- The length of the match, or ⊥ if it didn't match.
- The length of chars examined to make the match. To support invalidation when ApplyEdit.
- The parse result generated by the match.

===== Drawbacks
1. Doesn't improve asymptotic complexity of reparsing compared to the initial parsing. It's because the memo table is O(1) for indexing (id, pos) key but O(n) for interval overlap queries and apply shifts.
2. The memo strategy (memo every non-terminal) has high memory overhead and uses O(n) time when reparse with a flat grammar (that doesn't much nesting of non-terminals).

===== Potential improvements
1. Use interval trees for overlap queries. O(log n) time. With some augmentations (lazy shifts) to improve the performance of shifting.
2. Use tree memoization strategy.

===== The complexity of interval tree

- Find/Insert/Delete: O(log n)
- Query for all overlapping intervals with a specific interval: O(m + log n) where m is the result size

===== Lazy shifts

An edit at position stem:[n] need to shift all entries after stem:[n] by the number of characters inserted or deleted.
Optimization: lazy shifting. Store the shift to a global shift list when an edit occurs. Each node in the interval tree keeps a timestamp to track its most recently applied shift. Apply any more recent shifts from the global list when there's a data request to the node.

Alternative approaches:

- Fully lazy data structure where shifts are propagated to individual nodes.
- Rope-like DSs where interval position can be calculated from relative information.

===== Tree memoization
Special handle to the Kleene repetition operator (as it's the cause of linear structures in the memo table).
Modifications to memo table:

1. Multiple memo entries for the same ID(non-terminal) and position. Provide the result with the largest interval.
2. Store the count of the repetitions for those entries are created as a result of tree memo.

Requires a stack when dealing with the Kleene repetition operator.

=== Incremental LR Parsing
Tim A. Wagner and Susan L. Graham. 1998. Efficient and flexible incremental parsing. ACM Trans. Program. Lang. Syst. 20, 5 (Sept. 1998), 980–1013. https://doi.org/10.1145/293677.293678

The algorithm is used in tree-sitter.

==== Implementation Difficulty

It looks like the algorithm is much more complicated.
The algorithm is not fully described in the paper and requires explanation from other sources.

==== Note

Previous work for incremental parsing of LR(k) grammars relied on state matching.

.State matching
each node of the parse tree is associated with a state of the push-down automata.
When trying to reuse a subtree, the state of the subtree must match the current state of the automata.
If it's true and the lookahead items are valid, the parser can shift the subtree.
The validity of the lookahead items is checked by conservatively checking if the stem:[k]
terminal symbols following the subtree are matched between the current parse tree and the previous parse tree.

===== Drawbacks

- The space cost by the association of states to nodes.

- Restriction on the context that subtrees can be considered valid for reuse.
E.g., in LR(1) parsing table, there can be similar distinct states (i.e., different item sets with identical cores).
It can be the case that an incremental parser to discard a subtree can rebuild an isomorphic one with distinct state numbers.

===== Improvements by sentential-form parsing

A subtree representing is reusable in the current parse state stem:[\rightarrow]
the subtree except for the right-hand edge (which can be affected by the lookahead outside the tree)
can be reused.

- No states recorded in the parse tree nodes.
- Subtrees can be reused in any grammatically correct context.
- Lookahead validation is free by consuming the input.

===== How does it work?

The incremental parsing algorithm in this paper is working on a parse tree with user changes.

The paper doesn't mention how the parse tree is updated. From some other references, I think it can be summarized as follows:
Once the initial parse tree is built, the incremental lexer will update the parse tree when the user edits the source code.
It calculates lookback counts from the lookahead values to find all preceding tokens required to be included in the re-lexing phase.
Then, the lexer will re-lex from the furthest preceding node. It works at least till all tokens up to the edited token.
It may be the case that new tokens are generated; thus, the lexer only stops when it reaches a token that doesn't change after re-lexing.
The newly generated tokens will overwrite the processed tokens in the parse tree.
Extra nodes in the parse tree will be removed, or extra tokens will be added to the parse tree.

.Conditions of reusing(shifting) subtrees optimistically
- The subtree itself is unchanged.
- No other outstanding reductions on the parse stack.

As mentioned, it's still necessary to consider the lookahead items. This condition is checked lazily,
so the reusable subtrees are always shifted optimistically.

After the shifting, the parser will enter the validation phase, which will end when the next lookahead item can be shifted.
Otherwise, the optimistic shift will need to be handled further. To maximize the reuse, it's not preferable to just discard the subtree.
What the algorithm does is break down the subtree until the rightmost token. The procedure is called `right_breakdown`.

[source, python]
----
def right_breakdown():
 node = stack.pop() # remove optimistically shifted subtree
  while node is nonterminal:
    for c in node.children:
 action = parsetable.lookup(stack[-1].state, c.symbol)
 shift(c, action)
 node = stack.pop()
 action = parsetable.lookup(state[-1].state, node.symbol)
 shift(node, action) # leave final token on stack
----

===== Node reuse

There are subtrees that have not been reused and re-parsed, but they can still have some identical sub-subtrees that also exist in a previous parse tree.

.Bottom-up reuse
Upon a reduction, we check if the children being reduced already have a parent in the previous parse tree.
If the type of the old parent is the same as the type of the reduction, 
and all children share the same parent in both the previous and current version of the parse tree, then the old parent can be reused.

.Top-down reuse
Traverse the parse tree from top to bottom and compare any newly created node with the node that shares the same location in the previous version of the parse tree.
If they both have the same type (i.e., both have the same production symbol), then the new node can be replaced with the previous one.

== Incremental String Searching

1.Meyer, B. 1985. Incremental string matching. Information Processing Letters, 21(5), p.219–227.

2.Tsuda, K., Fuketa, M., and Aoe, J.I. 1995. An incremental algorithm for string pattern matching machines. International Journal of Computer Mathematics, 58(1-2), p.33–42.

==== Implementation Difficulty
The extensions to the AC algorithm are both simple.

==== Note

The non-incremental version of this problem is to search a set of strings (needles) in a piece of text (haystack).
The Aho–Corasick algorithm (a.k.a AC machine algorithm) constructs an automata for needles set before searching in the haystack.
By incremental, it means the needles can be subject to expanding as the search proceeds and the automata shouldn't be fully constructed from scratch.
The 2 papers both introduce approaches to incrementally construct the automata.

1. For Meyer's work, to incrementally build the automata, the incremental version of the algorithm needs to maintain extra data structures.
During the automata construction, AC algorithm keeps the longest proper suffix (lps) for every state in the trie.
With the incremental requirement, to update the lps of existing states, there should be a reverse map from the lps to the states.
The extra time complexity mainly originates from constructing this structure. It's O(K * the length of the longest needle * the total length of all needles).
K is the size of the character set.

2. For the later work, the authors think maintaining the extra data structure is a main drawback as the time cost and space cost can be large.
It's not clear what the authors exactly do. They can have a worst case time complexity of O(m + the length of the longest needle + the total length of all needles)
where m is the number of states in the automata. The worst case happens in some cases like inserting a common prefix of all existing needles.

== Incremental String Comparison

Yusuke Ishida, Shunsuke Inenaga, Ayumi Shinohara, and Masayuki Takeda. 2005. Fully incremental LCS computation. In Proceedings of the 15th international conference on Fundamentals of Computation Theory (FCT'05). Springer-Verlag, Berlin, Heidelberg, 563–574. https://doi.org/10.1007/11537311_49

==== Implementation Difficulty
It contains 4 sub-algorithms.
Each of sub-algorithms may contain one or multiple dynamic programming matrices.
The difficulty depends on if the implementers are familiar with dynamic programming.

==== Note
There is a common metric for string comparison, longest common subsequences (LCS).
The incremental computation here means that when knowing LCS(A, B), how to incrementally calculate the LCS after adding a character as a suffix or a prefix to one of the strings.
The sub-algorithms have different time complexities and space complexities as follows:

[cols="6,6", separator="!"]
!===
!Algorithm !Time Complexity
!`LCS(aA, B)`! `O(LCS(A, B))`
!`LCS(Aa, B)`! `O(LCS(A, B))`
!`LCS(A, bB)`! `O(|A|)`
!`LCS(A, Bb)`! `O(|A|)`
!`Total`     ! `O(LCS(A, B)*|A| + |B|)`
!===


== Incremental Inprocessing in SAT Solving

Fazekas, K., Biere, A., & Scholl, C. (2019). Incremental Inprocessing in SAT Solving. International Conference on Theory and Applications of Satisfiability Testing.

==== Implementation Difficulty

The technique is tightly coupled with the SAT solver. It may be hard to implement in a separate context.

==== Note
The processing technique in SAT solving means rewriting and simplifying formulas before the actual search for
satisfiability. It can be extended to interleave formula simplification and the search process, which is called
_inprocessing_.

The incremental SAT problem is defined as follows:
An incremental SAT problem stem:[F] is a sequence of clause sets stem:[〈∆_0, ..., ∆_n〉].
In phase stem:[i = 0, ..., n ], the task is to determine the satisfiability of stem:[F^i=∧_{s=0...i}∆_s], the conjunction of all added clauses up to this point.
If stem:[F_i] is unsatisfiable, then stem:[F_j] for all stem:[j > i] is unsatisfiable as well, as each iteration just augments the set of clauses.

The incremental SAT solving requires to efficiently reuse the already learned information. As an essential part
of the SAT solver, the inprocessing technique should be incremental as well.

The details are very niche to the SAT solving domain so it's hard to conclude the core idea here.

== Incremental Trace-based JIT Compilation

A\. Gal and M. Franz. Incremental Dynamic Code Generation with Trace Trees. Technical
Report ICS-TR-006-16, University of California, Irvine, 2006.

==== Implementation Difficulty

The techinique itself is simple, but I am not sure if it would be useful outside the JIT compilation background.

==== Note
The concept of incremental in this paper is different here compared to the previous examples. Here, the incremental procedure
is just about appending new compiled code to existing code (and reusing the existing code). It doesn't involve
invalidation or updating the existing code. Therefore, this topic could not be as interesting as the previous
ones.

As an early work of trace-based JIT compilation, this paper uses a simple but effective tree structure
to partially represent a program, compared to the full representation with a control flow graph.

The trace tree is constructed during the execution by interpreting the program. Once some conditions are
satisfied, the trace tree will be constructed and further extended.

The root of the trace tree is the so-called anchor node. It will be selected based on the frequency of execution.
In usual cases, it's most likely one of the inner-most
loop-header basic blocs in the program. After the root is selected, the JIT compiler will start to record the code
executed since the anchor node until there's a back edge to the anchor node, or the trace is too long.

If there's a side exit of which the path is not covered by the trace tree itself, the JTI compiler may start a new trace recording.
The new trace forms a branch from an existing node in the trace tree, so it's possible that it also shares a common prefix
of the trace tree.

There are other considerations on this topic, like how to effectively handle the performance-dominant loop structure with the
trace tree representation, how to do fast but optimized code compilation with the tree structure, etc. But generally they
are not directly related to the incremental compilation since the tree structure itself makes these computations easier.