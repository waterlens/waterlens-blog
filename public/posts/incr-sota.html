<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
  <link href="https://fonts.googleapis.com/css2?family=Oxygen:wght@400;700&amp;display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&amp;display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&amp;display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css" rel="stylesheet">
  <link rel="stylesheet" href="/style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
  </script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="Asciidoctor 2.0.23 with Waterlens HTML Backend 0.1.0">
  <title>Incremental Computation SOTAs</title>
</head>
<body>
  <article>
    <header>
      <h1><a href=".">Incremental Computation SOTAs</a></h1>
    </header>
    <hr>
    <div id="content">
      <section class="sect1">
        <h2 id="_incremental_parsing">Incremental Parsing</h2>
        <section class="sect2">
          <h3 id="_incremental_packrat_parsing">Incremental Packrat Parsing</h3>
          <p>Zachary Yedidia and Stephen Chong. 2021. Fast incremental PEG parsing. In Proceedings of the 14th ACM SIGPLAN International Conference on Software Language Engineering (SLE 2021). Association for Computing Machinery, New York, NY, USA, 99–112. <a href="https://doi.org/10.1145/3486608.3486900" class="bare">https://doi.org/10.1145/3486608.3486900</a></p>
          <section class="sect3">
            <h4 id="_implementation_difficulty">Implementation Difficulty</h4>
            <div class="olist arabic">
              <ol class="arabic">
                <li>
                  <p>Interval Tree: balanced binary search tree (AVL in prototype). Looks fine.</p>
                </li>
                <li>
                  <p>Tree Memo: a stack, scan and insert after each iteration. Looks fine.</p>
                </li>
                <li>
                  <p>Parser Machine: an abstract machine for parsing. Can be easy if supported features are limited.</p>
                </li>
              </ol>
            </div>
          </section>
          <section class="sect3">
            <h4 id="_note">Note</h4>
            <div class="listing">
              <div class="title">
                Naive Incremental Packrat Parsing Algorithm
              </div>
              <div class="content">
                <pre class="highlight"><code>M ← a new memoization table
s ← the initial input string
G ← the grammar
When an edit operation e occurs
  s, M ← ApplyEdit(s, M, e)
  M, res ← Parse(G, s, M)</code></pre>
              </div>
            </div>
            <p>Memo entries contain:</p>
            <div class="ulist">
              <ul>
                <li>
                  <p>The length of the match, or ⊥ if it didn’t match.</p>
                </li>
                <li>
                  <p>The length of chars examined to make the match. To support invalidation when ApplyEdit.</p>
                </li>
                <li>
                  <p>The parse result generated by the match.</p>
                </li>
              </ul>
            </div>
            <section class="sect4">
              <h5 id="_drawbacks">Drawbacks</h5>
              <div class="olist arabic">
                <ol class="arabic">
                  <li>
                    <p>Doesn’t improve asymptotic complexity of reparsing compared to the initial parsing. It’s because the memo table is O(1) for indexing (id, pos) key but O(n) for interval overlap queries and apply shifts.</p>
                  </li>
                  <li>
                    <p>The memo strategy (memo every non-terminal) has high memory overhead and uses O(n) time when reparse with a flat grammar (that doesn’t much nesting of non-terminals).</p>
                  </li>
                </ol>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_potential_improvements">Potential improvements</h5>
              <div class="olist arabic">
                <ol class="arabic">
                  <li>
                    <p>Use interval trees for overlap queries. O(log n) time. With some augmentations (lazy shifts) to improve the performance of shifting.</p>
                  </li>
                  <li>
                    <p>Use tree memoization strategy.</p>
                  </li>
                </ol>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_the_complexity_of_interval_tree">The complexity of interval tree</h5>
              <div class="ulist">
                <ul>
                  <li>
                    <p>Find/Insert/Delete: O(log n)</p>
                  </li>
                  <li>
                    <p>Query for all overlapping intervals with a specific interval: O(m + log n) where m is the result size</p>
                  </li>
                </ul>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_lazy_shifts">Lazy shifts</h5>
              <p>An edit at position \(n\) need to shift all entries after \(n\) by the number of characters inserted or deleted. Optimization: lazy shifting. Store the shift to a global shift list when an edit occurs. Each node in the interval tree keeps a timestamp to track its most recently applied shift. Apply any more recent shifts from the global list when there’s a data request to the node.</p>
              <p>Alternative approaches:</p>
              <div class="ulist">
                <ul>
                  <li>
                    <p>Fully lazy data structure where shifts are propagated to individual nodes.</p>
                  </li>
                  <li>
                    <p>Rope-like DSs where interval position can be calculated from relative information.</p>
                  </li>
                </ul>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_tree_memoization">Tree memoization</h5>
              <p>Special handle to the Kleene repetition operator (as it’s the cause of linear structures in the memo table). Modifications to memo table:</p>
              <div class="olist arabic">
                <ol class="arabic">
                  <li>
                    <p>Multiple memo entries for the same ID(non-terminal) and position. Provide the result with the largest interval.</p>
                  </li>
                  <li>
                    <p>Store the count of the repetitions for those entries are created as a result of tree memo.</p>
                  </li>
                </ol>
              </div>
              <p>Requires a stack when dealing with the Kleene repetition operator.</p>
            </section>
          </section>
        </section>
        <section class="sect2">
          <h3 id="_incremental_lr_parsing">Incremental LR Parsing</h3>
          <p>Tim A. Wagner and Susan L. Graham. 1998. Efficient and flexible incremental parsing. ACM Trans. Program. Lang. Syst. 20, 5 (Sept. 1998), 980–1013. <a href="https://doi.org/10.1145/293677.293678" class="bare">https://doi.org/10.1145/293677.293678</a></p>
          <p>The algorithm is used in tree-sitter.</p>
          <section class="sect3">
            <h4 id="_implementation_difficulty_2">Implementation Difficulty</h4>
            <p>It looks like the algorithm is much more complicated. The algorithm is not fully described in the paper and requires explanation from other sources.</p>
          </section>
          <section class="sect3">
            <h4 id="_note_2">Note</h4>
            <p>Previous work for incremental parsing of LR(k) grammars relied on state matching.</p>
            <div class="paragraph">
              <div class="title">
                State matching
              </div>
              <p>each node of the parse tree is associated with a state of the push-down automata. When trying to reuse a subtree, the state of the subtree must match the current state of the automata. If it’s true and the lookahead items are valid, the parser can shift the subtree. The validity of the lookahead items is checked by conservatively checking if the \(k\) terminal symbols following the subtree are matched between the current parse tree and the previous parse tree.</p>
            </div>
            <section class="sect4">
              <h5 id="_drawbacks_2">Drawbacks</h5>
              <div class="ulist">
                <ul>
                  <li>
                    <p>The space cost by the association of states to nodes.</p>
                  </li>
                  <li>
                    <p>Restriction on the context that subtrees can be considered valid for reuse. E.g., in LR(1) parsing table, there can be similar distinct states (i.e., different item sets with identical cores). It can be the case that an incremental parser to discard a subtree can rebuild an isomorphic one with distinct state numbers.</p>
                  </li>
                </ul>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_improvements_by_sentential_form_parsing">Improvements by sentential-form parsing</h5>
              <p>A subtree representing is reusable in the current parse state \(\rightarrow\) the subtree except for the right-hand edge (which can be affected by the lookahead outside the tree) can be reused.</p>
              <div class="ulist">
                <ul>
                  <li>
                    <p>No states recorded in the parse tree nodes.</p>
                  </li>
                  <li>
                    <p>Subtrees can be reused in any grammatically correct context.</p>
                  </li>
                  <li>
                    <p>Lookahead validation is free by consuming the input.</p>
                  </li>
                </ul>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_how_does_it_work">How does it work?</h5>
              <p>The incremental parsing algorithm in this paper is working on a parse tree with user changes.</p>
              <p>The paper doesn’t mention how the parse tree is updated. From some other references, I think it can be summarized as follows: Once the initial parse tree is built, the incremental lexer will update the parse tree when the user edits the source code. It calculates lookback counts from the lookahead values to find all preceding tokens required to be included in the re-lexing phase. Then, the lexer will re-lex from the furthest preceding node. It works at least till all tokens up to the edited token. It may be the case that new tokens are generated; thus, the lexer only stops when it reaches a token that doesn’t change after re-lexing. The newly generated tokens will overwrite the processed tokens in the parse tree. Extra nodes in the parse tree will be removed, or extra tokens will be added to the parse tree.</p>
              <div class="ulist">
                <div class="title">
                  Conditions of reusing(shifting) subtrees optimistically
                </div>
                <ul>
                  <li>
                    <p>The subtree itself is unchanged.</p>
                  </li>
                  <li>
                    <p>No other outstanding reductions on the parse stack.</p>
                  </li>
                </ul>
              </div>
              <p>As mentioned, it’s still necessary to consider the lookahead items. This condition is checked lazily, so the reusable subtrees are always shifted optimistically.</p>
              <p>After the shifting, the parser will enter the validation phase, which will end when the next lookahead item can be shifted. Otherwise, the optimistic shift will need to be handled further. To maximize the reuse, it’s not preferable to just discard the subtree. What the algorithm does is break down the subtree until the rightmost token. The procedure is called <code>right_breakdown</code>.</p>
              <div class="listing">
                <div class="content">
                  <pre class="highlight"><code class="language-python" data-lang="python">def right_breakdown():
 node = stack.pop() # remove optimistically shifted subtree
  while node is nonterminal:
    for c in node.children:
 action = parsetable.lookup(stack[-1].state, c.symbol)
 shift(c, action)
 node = stack.pop()
 action = parsetable.lookup(state[-1].state, node.symbol)
 shift(node, action) # leave final token on stack</code></pre>
                </div>
              </div>
            </section>
            <section class="sect4">
              <h5 id="_node_reuse">Node reuse</h5>
              <p>There are subtrees that have not been reused and re-parsed, but they can still have some identical sub-subtrees that also exist in a previous parse tree.</p>
              <div class="paragraph">
                <div class="title">
                  Bottom-up reuse
                </div>
                <p>Upon a reduction, we check if the children being reduced already have a parent in the previous parse tree. If the type of the old parent is the same as the type of the reduction, and all children share the same parent in both the previous and current version of the parse tree, then the old parent can be reused.</p>
              </div>
              <div class="paragraph">
                <div class="title">
                  Top-down reuse
                </div>
                <p>Traverse the parse tree from top to bottom and compare any newly created node with the node that shares the same location in the previous version of the parse tree. If they both have the same type (i.e., both have the same production symbol), then the new node can be replaced with the previous one.</p>
              </div>
            </section>
          </section>
        </section>
      </section>
      <section class="sect1">
        <h2 id="_incremental_string_searching">Incremental String Searching</h2>
        <p>1.Meyer, B. 1985. Incremental string matching. Information Processing Letters, 21(5), p.219–227.</p>
        <p>2.Tsuda, K., Fuketa, M., and Aoe, J.I. 1995. An incremental algorithm for string pattern matching machines. International Journal of Computer Mathematics, 58(1-2), p.33–42.</p>
        <h4 id="_implementation_difficulty_3" class="discrete">Implementation Difficulty</h4>
        <p>The extensions to the AC algorithm are both simple.</p>
        <h4 id="_note_3" class="discrete">Note</h4>
        <p>The non-incremental version of this problem is to search a set of strings (needles) in a piece of text (haystack). The Aho–Corasick algorithm (a.k.a AC machine algorithm) constructs an automata for needles set before searching in the haystack. By incremental, it means the needles can be subject to expanding as the search proceeds and the automata shouldn’t be fully constructed from scratch. The 2 papers both introduce approaches to incrementally construct the automata.</p>
        <div class="olist arabic">
          <ol class="arabic">
            <li>
              <p>For Meyer’s work, to incrementally build the automata, the incremental version of the algorithm needs to maintain extra data structures. During the automata construction, AC algorithm keeps the longest proper suffix (lps) for every state in the trie. With the incremental requirement, to update the lps of existing states, there should be a reverse map from the lps to the states. The extra time complexity mainly originates from constructing this structure. It’s O(K * the length of the longest needle * the total length of all needles). K is the size of the character set.</p>
            </li>
            <li>
              <p>For the later work, the authors think maintaining the extra data structure is a main drawback as the time cost and space cost can be large. It’s not clear what the authors exactly do. They can have a worst case time complexity of O(m + the length of the longest needle + the total length of all needles) where m is the number of states in the automata. The worst case happens in some cases like inserting a common prefix of all existing needles.</p>
            </li>
          </ol>
        </div>
      </section>
      <section class="sect1">
        <h2 id="_incremental_string_comparison">Incremental String Comparison</h2>
        <p>Yusuke Ishida, Shunsuke Inenaga, Ayumi Shinohara, and Masayuki Takeda. 2005. Fully incremental LCS computation. In Proceedings of the 15th international conference on Fundamentals of Computation Theory (FCT'05). Springer-Verlag, Berlin, Heidelberg, 563–574. <a href="https://doi.org/10.1007/11537311_49" class="bare">https://doi.org/10.1007/11537311_49</a></p>
        <h4 id="_implementation_difficulty_4" class="discrete">Implementation Difficulty</h4>
        <p>It contains 4 sub-algorithms. Each of sub-algorithms may contain one or multiple dynamic programming matrices. The difficulty depends on if the implementers are familiar with dynamic programming.</p>
        <h4 id="_note_4" class="discrete">Note</h4>
        <p>There is a common metric for string comparison, longest common subsequences (LCS). The incremental computation here means that when knowing LCS(A, B), how to incrementally calculate the LCS after adding a character as a suffix or a prefix to one of the strings. The sub-algorithms have different time complexities and space complexities as follows:</p>
        <table class="table frame-all grid-all stretch">
          <colgroup>
            <col style="width: 50%;">
            <col style="width: 50%;">
          </colgroup>
          <tbody>
            <tr>
              <td class="table halign-left valign-top">
                <p class="table">Algorithm</p>
              </td>
              <td class="table halign-left valign-top">
                <p class="table">Time Complexity</p>
              </td>
            </tr>
            <tr>
              <td class="table halign-left valign-top">
                <p class="table"><code>LCS(aA, B)</code></p>
              </td>
              <td class="table halign-left valign-top">
                <p class="table"><code>O(LCS(A, B))</code></p>
              </td>
            </tr>
            <tr>
              <td class="table halign-left valign-top">
                <p class="table"><code>LCS(Aa, B)</code></p>
              </td>
              <td class="table halign-left valign-top">
                <p class="table"><code>O(LCS(A, B))</code></p>
              </td>
            </tr>
            <tr>
              <td class="table halign-left valign-top">
                <p class="table"><code>LCS(A, bB)</code></p>
              </td>
              <td class="table halign-left valign-top">
                <p class="table"><code>O(|A|)</code></p>
              </td>
            </tr>
            <tr>
              <td class="table halign-left valign-top">
                <p class="table"><code>LCS(A, Bb)</code></p>
              </td>
              <td class="table halign-left valign-top">
                <p class="table"><code>O(|A|)</code></p>
              </td>
            </tr>
            <tr>
              <td class="table halign-left valign-top">
                <p class="table"><code>Total</code></p>
              </td>
              <td class="table halign-left valign-top">
                <p class="table"><code>O(LCS(A, B)*|A| + |B|)</code></p>
              </td>
            </tr>
          </tbody>
        </table>
      </section>
      <section class="sect1">
        <h2 id="_incremental_inprocessing_in_sat_solving">Incremental Inprocessing in SAT Solving</h2>
        <p>Fazekas, K., Biere, A., & Scholl, C. (2019). Incremental Inprocessing in SAT Solving. International Conference on Theory and Applications of Satisfiability Testing.</p>
        <h4 id="_implementation_difficulty_5" class="discrete">Implementation Difficulty</h4>
        <p>The technique is tightly coupled with the SAT solver. It may be hard to implement in a separate context.</p>
        <h4 id="_note_5" class="discrete">Note</h4>
        <p>The processing technique in SAT solving means rewriting and simplifying formulas before the actual search for satisfiability. It can be extended to interleave formula simplification and the search process, which is called <em>inprocessing</em>.</p>
        <p>The incremental SAT problem is defined as follows: An incremental SAT problem \(F\) is a sequence of clause sets \(〈∆_0, ..., ∆_n〉\). In phase \(i = 0, ..., n \), the task is to determine the satisfiability of \(F^i=∧_{s=0...i}∆_s\), the conjunction of all added clauses up to this point. If \(F_i\) is unsatisfiable, then \(F_j\) for all \(j &gt; i\) is unsatisfiable as well, as each iteration just augments the set of clauses.</p>
        <p>The incremental SAT solving requires to efficiently reuse the already learned information. As an essential part of the SAT solver, the inprocessing technique should be incremental as well.</p>
        <p>The details are very niche to the SAT solving domain so it’s hard to conclude the core idea here.</p>
      </section>
      <section class="sect1">
        <h2 id="_incremental_trace_based_jit_compilation">Incremental Trace-based JIT Compilation</h2>
        <p>A\. Gal and M. Franz. Incremental Dynamic Code Generation with Trace Trees. Technical Report ICS-TR-006-16, University of California, Irvine, 2006.</p>
        <h4 id="_implementation_difficulty_6" class="discrete">Implementation Difficulty</h4>
        <p>The techinique itself is simple, but I am not sure if it would be useful outside the JIT compilation background.</p>
        <h4 id="_note_6" class="discrete">Note</h4>
        <p>The concept of incremental in this paper is different here compared to the previous examples. Here, the incremental procedure is just about appending new compiled code to existing code (and reusing the existing code). It doesn’t involve invalidation or updating the existing code. Therefore, this topic could not be as interesting as the previous ones.</p>
        <p>As an early work of trace-based JIT compilation, this paper uses a simple but effective tree structure to partially represent a program, compared to the full representation with a control flow graph.</p>
        <p>The trace tree is constructed during the execution by interpreting the program. Once some conditions are satisfied, the trace tree will be constructed and further extended.</p>
        <p>The root of the trace tree is the so-called anchor node. It will be selected based on the frequency of execution. In usual cases, it’s most likely one of the inner-most loop-header basic blocs in the program. After the root is selected, the JIT compiler will start to record the code executed since the anchor node until there’s a back edge to the anchor node, or the trace is too long.</p>
        <p>If there’s a side exit of which the path is not covered by the trace tree itself, the JTI compiler may start a new trace recording. The new trace forms a branch from an existing node in the trace tree, so it’s possible that it also shares a common prefix of the trace tree.</p>
        <p>There are other considerations on this topic, like how to effectively handle the performance-dominant loop structure with the trace tree representation, how to do fast but optimized code compilation with the tree structure, etc. But generally they are not directly related to the incremental compilation since the tree structure itself makes these computations easier.</p>
      </section>
    </div>
    <hr>
    <footer>
      <p>The content on <a property="dct:title" rel="cc:attributionURL" href="/">this website</a> © 2021 - 2025 by <span property="cc:attributionName">Waterlens</span> is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0 <img alt="" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img alt="" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> <img alt="" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
    </footer>
  </article>
</body>
</html>
